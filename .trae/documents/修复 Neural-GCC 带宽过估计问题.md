## 问题根因：
1. **延迟惩罚失效**：当前奖励函数使用 10s 作为延迟归一化基准，导致 200-500ms 的严重延迟在模型看来“无关痛痒”。
2. **吞吐量权重过高**：模型为了追求极致的吞吐量，不惜以牺牲延迟和丢包为代价。
3. **缺乏过估计反馈**：没有机制告知模型“预测值 > 实际能力”是错误的。

## 修改步骤：
### 1. 优化 `_calculate_reward` 函数
- **重塑延迟惩罚**：将延迟惩罚的基准从 `max_delay (10s)` 改为 `threshold_delay (200-400ms)`。超过此阈值的延迟将受到剧烈惩罚。
- **引入过估计惩罚**：当 `current_prediction > receiving_rate` 且延迟/丢包上升时，增加额外的负反馈。
- **调整权重比例**：提高延迟和丢包的惩罚权重，降低吞吐量的基础权重。

### 2. 优化特征处理
- 检查 `_normalize_features` 是否有导致模型无法感知拥塞信号的缺陷。

### 3. 验证与微调
- 建议将 `min_bw` 和 `max_bw` 的逻辑与 `Config` 彻底解耦，或在奖励函数中使用更严苛的裁剪。
