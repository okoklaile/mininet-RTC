## 任务目标

确认并保持当前的“稳健微调”逻辑，确保 Neural-GCC 在提升性能的同时不丢失 BC-GCC 的稳定性。

## 技术要点

1. **参考模型锚定**：利用 `self.ref_model` 锁定预训练知识。
2. **KL 散度约束**：在 PPO 损失函数中集成 KL 惩罚，限制策略漂移。
3. **奖励函数协同**：配合动态 `target_delay` 和过估计惩罚，形成多重保护。

## 后续操作建议

1. **观察训练日志**：关注 `kl_loss` 的数值，如果该数值过大，说明 RL 正在剧烈挑战 BC 的先验。
2. **微调系数**：如果效果依然不够稳定，可以尝试将 KL 惩罚系数从 `0.2` 调高至 `0.5`。

